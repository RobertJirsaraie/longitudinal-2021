---
title: "mlm"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: xaringan-themer.css
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#23395b",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Droid Mono"),
)

library(tidyverse)
library(broom)
library(tidybayes)
library(modelr)
```


## MLM 
aka HLM, aka mixed effects, aka random effects   

Why? BC we would  
1. violate standard regression assumptions  
2. because of the flexibility  

Splits the model into two components:   
mean and variance   
location and scale  
fixed and random  
constant (across people) and varying (across people)  

---
## Terminology 

General Linear Models  
GeneralIZED Linear Models  
General Linear Mixed Models  
GeneralIZED Linear Mixed Models  
  
Mixed means both fixed and random effects   
"...IZED" means gaussian and other data generating processes 


---
## Modeling dependency

We have multiple DVs per person with longitudinal data. If we ignored the person aspect, the residuals would likely be related, violating standard regression assumption. MLM accounts for residuals for outcomes from the same person through modeling different "levels" (of random effects).

To have a “level”, there must be random outcome variation left over. With longitudinal data we have people nested in observations. 

Level 1: observation level (observation specific variance)  
Level 2: person level (person specific variance)  

---
### Person specific variance
.pull-left[
Some people start at different levels and some people change at different rates
]

.pull-right[
```{r, echo = FALSE, warning = FALSE, message = FALSE}

example <- read_csv("~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/example.csv")
example$year <- example$week


ggplot(example, aes(x = year, y = SMN7, group = ID, colour = ID)) + stat_smooth(method = "lm", se = FALSE, alpha = .5) +scale_color_viridis()
```
]

---
### Observation level variance
.pull-left[
After account for a person starting level and their slope, there is still residual variance left over. 
]

.pull-right[
```{r, echo = FALSE, message= FALSE, warning=FALSE}
ob.var <- example %>% 
  filter(ID %in% c("67","82", "110")) 

example3 <-
  left_join(ob.var, example)  
  
ggplot(example3,
   aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method="lm") + facet_wrap( ~ID)
```
]


---
## Thinking about variation 
A goal of longitudinal data analysis (and all other data analysis) is to explain this variation. We will fit models that are more constrained (makes assumptions about the shape of change across people) to see if that increases or reduces variation at the two levels. 

To the extent that we can put variance into different "piles" (eg people change at different rates) we will have more explained variance and less unexplained variance. Less unexplained variance means our model fits better ie it represents our data better

---
## Speaking of variation 

Between-Person (BP) Variation:  
Level-2 – “INTER-individual Differences” - Time-Invariant   
BP = More/less than other people  


Within-Person (WP) Variation:  
Level-1 – “INTRA-individual Differences” – Time-Varying  
WP = more/less than one’s average  

Any variable measured over time usually has both BP and WP variation. People who vary in positive emotion may be higher in PE. If we did not separate the two and only looked at WP then we will introduce bias. 


---
### Within person focus

Within-Person Change: Systematic change
Magnitude or direction of change can be different across individuals. Can refer to between person (inter individual differences) in within person change (intra individual)

Within-Person Fluctuation: No systematic change
Outcome just varies/fluctuates over time (e.g., emotion, stress). Time is just a way to get lots of data per individual


---
## Terms to know

Within person vs between person 
time varying vs time invariant
intra-individual vs inter-individual
observation level vs person level variance

(notice that there are often two levels to these)

---
## mlm review


$${Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... +\epsilon_{i}$$

$$\hat{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+...$$

Parameters are considered fixed where one regression value corresponds to everyone. I.e., that association between X1 and Y is the same for everyone. 

Each person has a Y, denoted by the subscript i, and each has a residual associated with them, also designated by i. 

---
## Handling multiple DVs? 

But what if people had more than 1 DV, like we do with time? What do you do with multiple items, multiple trials, multiple ____ ? 

Two options: 1. Collapse and average across. 


---
## Example
```{r, message = FALSE}
library(tidyverse)

simp<- tribble(
  ~ID, ~group,  ~test.score, ~study,
1,1,5,1,
2,1,7,3,
3,2,4,2,
4,2,6,4,
5,3,3,3,
6,3,5,5,
7,4,2,4,
8,4,4,6,
9,5,1,5,
10,5,3,7)
```

---
```{r, echo=FALSE}
simp %>% 
ggplot(aes(x=study, y=test.score)) +
    geom_point(aes(colour = factor(group)))
```

---
## could aggragate across group
```{r, echo = FALSE}
simp.1<- tribble(
  ~ID, ~group,  ~test.score, ~study,
  1,1,6,2,
  2,2,5,3,
  3,3,4,4,
  4,4,3,5,
  5,5,2,6)
```


```{r, echo=FALSE}
simp.1 %>% 
ggplot(aes(x=study, y=test.score)) +
    geom_point() +    
    geom_smooth(method=lm, se=FALSE) +
  geom_point(data = simp, aes(colour = factor(group)))
```

---

## Handling multiple DVs? 

But what if people had more than 1 DV, like we do with time? What do you do with multiple items, multiple trials, multiple ____ ?  

Two options: 1. Collapse and average across.  
2. use all available information

---

```{r, echo = FALSE}
simp %>% 
ggplot(aes(x=study, y=test.score, group = group)) +
    geom_point(aes(colour = factor(group))) +   
    geom_smooth(aes(colour = factor(group)),method=lm,se=FALSE)
```

---
## Aggregation obscures hypotheses

Between person H1: Do students who study more get better grades?   

Within person H2: When a student studies, do they get better grades?
			        	
H1 and H2 are independent from one another!


---
## Stroop example

We calculate stroop scores by looking at repeated trials of congruent vs not congruent. This is dummy coded such that the $\beta_{1}$ reflects the average stroop effect. How much slower are people in incongruent trials? 

$$Y_{i} = \beta_{0} + \beta_{1}X_{1} + \varepsilon_i$$

What if we ran a separate regression for everyone? We can then think of $\beta_{1}$ as a PERSON SPECIFIC EFFECT. What is the stoop effect for you? 

We could think of $\beta_{1}$ as a random variable where people deviate on stroop effect from the average (ie fixed effect). You can then treat this as a regression, complete with a residual. 

$$\beta_{1} = \gamma_{10} +\gamma_{11}Z_i+ U_{1i}$$

---
## 4 ways to think about MLMs

1. Different levels of analysis (average/person specific or between/within)  
2. Regressions within regressions (ie coefficients as outcomes)  
3. Variance decomposition  
4. Learning from other data through pooling/shrinkage  

---
### regressions within regressions 
Helps to take multilevel and split it into the different levels. 

Level 1 is the smallest unit of analysis (students, waves, trials, family members)
    
Level 2 variables are what level 1 variables are “nested” in (people, schools, counties, families, dyads)

We are going to use level one components to run a regression, all the while level 1 is also estimating a regression


---
L1
$$Y_{trials, i} = \beta_{0i} + \beta_{1i}X_{trial,i} + \varepsilon_{trial}$$
      
L2
$$\beta_{0} = \gamma_{00} + U_{0i}$$
$$\beta_{1} = \gamma_{10} +\gamma_{11}Age_i+ U_{1i}$$
Our B1 coefficient indexes the stroop effect. However, people differ on this stroop effect. There is some average effect (fixed effect) that people vary around. Each person has some personal $\beta_1$, which we find using Level 1 data. From there we can also ask questions (with regressions) about that random variable. Is age associated with stroop scores, for example.

---
People differ on the stroop. 
```{r, echo = FALSE, message=FALSE, warning = FALSE}

set.seed(11)
ex.random <- example %>% 
  dplyr::select(ID) %>% 
  distinct %>% 
  sample_n(3) 

example2 <-
  left_join(ex.random, example)  
  
g2<- ggplot(example2,
   aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method="lm", formula=y~1, se = FALSE) + facet_wrap( ~ID) +
  geom_hline(yintercept = .13) +  ylab("stroop effect") + xlab("trials") +
  geom_label(label="Grand mean ",  x=1,y=.13,
    label.size = 0.15) 
g2
```

```{r, echo=FALSE}
example %>% group_by(ID) %>%
  do(avg = tidy(lm(SMN7 ~ 1, data = .))) %>% 
  unnest(avg) %>% 
  ggplot(aes(x = estimate)) +
  stat_dotsinterval() + ylab("density")
```

---
### diferent levels and regressions within regressions 

To sum up the first two ways to think about regression, we take a relationship that could be simplified by aggregation, but instead we ask regression questions at different levels of analysis.  

At level 1 we can ask lower-unit questions e.g., if trials are nested within person, what predicts lengthier trials? 

At level 2 we can ask broader-unit questions. E.g., is age associated with stroop differences

Both levels are simple regressions. Level 2 uses coefficients from level 1 as DVs. 


---
### variance decomposition

For standard regression, we think of error as existing in one big bucket called $\varepsilon$ . Everything that we do not know goes into that bucket, from measurement error to unmeasured important factors. 

For MLMs we will be breaking up unexplained variance ($\varepsilon$) into multiple buckets. Some of these buckets will be useful variance.  

These useful "buckets" (Us) are what we refer to as random/varying effects. 


$$Y_{trials, i} = \beta_{0i} + \beta_{1i}X_{trial,i} + \varepsilon_{trial}$$
 $$\beta_{0} = \gamma_{00} + U_{0i}$$     
$$\beta_{1} = \gamma_{10} +\gamma_{11}Z_i+ U_{1i}$$


---

.pull-left[

![]btw.png)


]

.pull-right[


![](win.png)

]
 
---
Random effects used to be error, but they are going to be useful going forward. 

We will treat them as variables themselves e.g. individual differences in how people change  

They index how much people DIFFER on some effect. e.g. does everyone change the same, or are there differences in how people change?  

We can relate the random effects to other random effects e.g., do people who increase on X also start higher on X.   
 
 
---
### shrinkage/partial pooling

We treat our group variable as coming from a population. All groups are alike (because they are from the same population), but different in their own way. Because of this it is helpful to use information from other groups to help make predictions. 

We do this in standard regression where we make predictions based on values not only using data from X but from the whole dataset. A similar analogy for MLM is running a regression for each group. We want to pool as this leads to better predictions as we are not overfitting our data! 

---

If we take our simplified stroop effect model, where we are only looking at reaction time as a DV (ignoring the different types of trials) we could fit a model like this (an empty model)

L1
$$Y_{trials, i} = \beta_{0i} +  \varepsilon_{trial}$$
L2
$$\beta_{0} = \gamma_{00} + U_{0i}$$
 
Where does $U_{0i}$ come from? If we calculated each by hand, through taking the average rection time for a person i and subtracting that from the grand mean reaction time, would that equal $U_{0i}$ ?
 
---
## Complete, partial and no pooling

Complete assumes everyone is the same, with $U_{0i}$ being zero for everyone. 

No pooling is if we calculate every person's effect with a regression, subtracting out he grand mean average. 

Partial pooling is in the middle, a weighted average between the two. For those with fewer trials there is less information for a particular individual, thus the complete pooling estimate will be given more weight. If someone has a lot of data, there weighted average is closer to no pooling. 

Because of partial pooling these individual estimates are bettter as this procedure prevents over and under fitting of your data, leading to increased out of sample predictions. 

 
---
# Basic Models

To keep with the book, we are going to discuss DVs that take on different values at each timepoint t, for individual i ${Y}_{ti}$ Other naming schemes are equivalent such as the same ${Y}_{ij}$ where i's are nested in j groups. 

---
### Empty model

Level 1
$${Y}_{ti} = \beta_{0i}  + \varepsilon_{ti}$$

Level 2
$${\beta}_{0i} = \gamma_{00} + U_{0i}$$
 

$${e}_{ti} \sim \mathcal{N}(0, \sigma^{2})$$

$${U}_{0i} \sim \mathcal{N}(0, \tau_{00}^{2})$$

---
What does this look like? 

```{r, echo = FALSE}
library(tidyverse)
sample_n_of <- function(data, size, ...) {
  dots <- quos(...)
  
  group_ids <- data %>% 
    group_by(!!! dots) %>% 
    group_indices()
  
  sampled_groups <- sample(unique(group_ids), size)
  data %>% 
    filter(group_ids %in% sampled_groups)
}
```

```{r, echo = FALSE, warning = FALSE}
set.seed(24)


example %>%
  sample_n_of(8, ID) %>% 
ggplot(aes(x = week, y = CON, group = ID)) + geom_point(aes(color = factor(ID))) + stat_smooth(aes(color = factor(ID)), method = "lm", formula=y~1, se = FALSE) + xlab("We dont have a predictor") + ylab("Y") + theme(legend.position = "none") + geom_hline(yintercept = .22, size = 1.5)
```

---
combined equation

$${Y}_{ti} = \gamma_{00} + U_{0i}  + \varepsilon_{ti}$$

Akin to ANOVA if we treat $U_{0i}$ as between subjects variance & $\varepsilon_{ti}$ as within subjects variance.


$\gamma_{00} + U_{0i}$ is fixed or constant across people
$U_{0i}$ is random or varies across people

---
## ICC

Between version variance over total variance. If the ICC is greater than zero, we are breaking standard regression assumptions as this is an index of dependency. 

$$\frac{U_{0i}}{U_{0i}+ \varepsilon_{ti}}$$

ICC can also be interpreted as the average (or expected) correlation within a nested group, in this case a person. On other words, the ICC is the correlation between any person's repeated measures (technically residuals).


---
## Time in level 1 


Level 1 is where you have data that repeats within your grouping or clustering data. Is your cluster classrooms? Then students are level 1. Is your cluster people? Then observations are level 1. 

$${Y}_{ti} = \beta_{0i}  + \beta_{1i}X_{ti} + \varepsilon_{ti}$$

Notice on the subscript of X that these predictors vary across group (i) and within the group (t) So if your grouping (i) is people, then t refers to different observations.

This is a simple growth curve model. It fits linear association for time


---

Level 2 takes the parameters at level 1 and decomposes them into a fixed component ($\gamma$) that reflects the average and, if desired, the individual deviations around that fixed effect (U).

level 1
$${Y}_{ti} = \beta_{0i}  + \beta_{1i}X_{ti} + \varepsilon_{ti}$$

level 2
$${\beta}_{0i} = \gamma_{00} + U_{0i}$$  
$${\beta}_{1i} = \gamma_{10}$$  

---

What implied association between X and Y does this model fit for each person? What is constrained to be the same vs different? 

---
Including a random slope

level 1
$${Y}_{ti} = \beta_{0i}  + \beta_{1i}X_{ti} + \varepsilon_{ti}$$

level 2
$${\beta}_{0i} = \gamma_{00} + U_{0i}$$  
$${\beta}_{1i} = \gamma_{10}+ U_{1i}$$ 
combined: 
$${Y}_{ti} = \gamma_{00} + \gamma_{10}(X_{ti})+ U_{0i} + U_{1i}(X_{ti}) + \varepsilon_{ti}$$

---


By including random effects (U) you making a claim that every group/cluster does *not* have the same $\gamma$ ie intercept/regression coefficient. 

An advantage of MLM is to separate more "buckets" of variance that are unexplained. What was originally $e_ti$ is now ( $U_0$ + $U_1$ + $e_ti$ ). This additional decomposition of variance is beneficial because you are separating signal from noise, translating what was noise $e_ij$ into meaningful signal ( $U_0$ , $U_1$ , $U_2$ etc  ). 

For example, multiple responses per person can identify individual differences (eg not everyone shows the stroop effect) that normally would be chalked up to error. If you parse out this error your signal becomes stronger.

---
## person predictions

Can think of a persons score divided up into a fixed component as well as the random component. 

$${\beta}_{1.26} = \gamma_{10} \pm U_{26}$$ 

---
## Error Structure
The residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance. 

G matrix (books term)
$$\begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,      \tau_{00}^{2} & \tau_{01}\\ 
  0,  \tau_{01} & \tau_{10}^{2}
\end{pmatrix}$$
 


R matrix 
$${e}_{ij} \sim \mathcal{N}(0, \sigma^{2})$$
 
Note that it is possible to have a different error structure. Chapter 4 (optional talks about this, and we will go into more detail later in the semester)

---
## centering 



---
## Estimation 



---
## pseudo R2


---
## categorical estimation 

---
## transitioning to longitudinal applications



---
## between person 



---
## within person empty model


---
##