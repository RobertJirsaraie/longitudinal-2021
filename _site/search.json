{
  "articles": [
    {
      "path": "about.html",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-01-13T13:45:39-06:00"
    },
    {
      "path": "data.html",
      "title": "Working with data",
      "author": [],
      "contents": "\n\nContents\nWhat Are Data?\nWide vs long\ntidyr\npivot_wider\npivot_longer\nSeperate and Unite\ndate time metrics\n\nProjects and Rmarkdown\nPackages\nCodebook\nData\n\nClean Data\nRecode Variables\nReverse-Scoring\nCreate Composites\n\nDescriptives\nmetric variables\ncount variables\nZero-Order Correlations\n\n\nWhat Are Data?\nData are the core of everything that we do in statistical analysis. Data come in many forms, and I don’t just mean .csv, .xls, .sav, etc. Data can be wide, long, documented, fragmented, messy, and about anything else that you can imagine.\nAlthough data could arguably be more means than end in psychology, the importance of understanding the structure and format of your data cannot overstated. Failure to understand your data could end in improper techniques and flagrantly wrong inferences at worst. This is especially important for longitudinal data. We will discuss many aspects of data handling. One thing to note is that this is just ONE WAY to do it. There are many equivalent.\nWide vs long\nAka multivariate vs stacked; person vs person period untidy vs tidy*\nLong is what MLM, ggplot2 and tidyverse packages expect whereas SEM and a lot of descriptive are calculated using wide dataframes.\n\nIn this figure X, Y, and Z could represent different waves of collection. For each wave we have some value for each of the two people in the dataset. In the long format each person has each wave as a separate row. In the wide each person has their data on a single row.\nWe will be working with long data for the first half of the class and wide data the second. However, even during the first half we will need to switch back and forth to make sure we can calculate certain values.\ntidyr\nThe best package to go back and forth between long and wide is the tidyr package, which is part of the tidyverse. Here we will walk through some examples of the primary functions, pivot_wider and pivot_longer\n\n\n\nFor longitudinal/repeated measures data, each row is an observation. Each person will have multiple rows. You can grab some example data from the class’s github\n\n     ID wave group    DAN\n1     6    1    PD 0.1619\n2     6    2    PD 0.1677\n3     6    3    PD 0.2153\n4    29    1    PD 0.1749\n5    29    2    PD 0.1356\n6    34    1  CTRL 0.1659\n7    34    2  CTRL 0.1403\n8    36    1  CTRL 0.1522\n9    36    2  CTRL 0.2053\n10   37    1    PD 0.2194\n11   37    2    PD 0.1579\n12   37    3    PD 0.2586\n13   48    1    PD 0.1302\n14   48    2    PD 0.2703\n15   48    3    PD 0.2478\n16   53    1  CTRL 0.2112\n17   53    2  CTRL 0.1521\n18   54    1    PD 0.2205\n19   54    2    PD 0.1521\n20   54    3    PD 0.1920\n21   58    1    PD 0.3801\n22   58    2    PD 0.2148\n23   58    3    PD 0.2036\n24   61    1    PD 0.0818\n25   61    2    PD 0.0628\n26   66    1  CTRL 0.1879\n27   66    2  CTRL 0.1476\n28   66    3  CTRL 0.1975\n29   67    1    PD 0.1759\n30   67    2    PD 0.1418\n31   67    3    PD 0.1834\n32   67    4    PD 0.1464\n33   69    1  CTRL 0.1775\n34   69    2  CTRL 0.1479\n35   71    1    PD 0.0902\n36   71    2    PD 0.1852\n37   71    3    PD 0.0733\n38   74    1  CTRL 0.2094\n39   74    2  CTRL 0.2185\n40   74    3  CTRL 0.2015\n41   75    1  CTRL 0.2396\n42   75    2  CTRL 0.1872\n43   76    1    PD 0.2393\n44   76    2    PD 0.1656\n45   76    3    PD 0.2584\n46   78    1  CTRL 0.1557\n47   78    2  CTRL 0.2599\n48   78    3  CTRL 0.1434\n49   79    1    PD 0.2288\n50   79    2    PD 0.2267\n51   80    1    PD 0.1893\n52   80    2    PD 0.1930\n53   81    1  CTRL 0.1103\n54   81    2  CTRL 0.1196\n55   81    3  CTRL 0.1051\n56   82    1    PD 0.2272\n57   82    2    PD 0.2124\n58   82    3    PD 0.2547\n59   82    4    PD 0.2063\n60   85    1    PD 0.2566\n61   85    2    PD 0.2079\n62   86    1    PD 0.1778\n63   86    2    PD 0.1943\n64   87    1    PD 0.1417\n65   87    2    PD 0.0803\n66   89    1    PD 0.2306\n67   89    2    PD 0.3029\n68   89    3    PD 0.1503\n69   91    1    PD 0.2962\n70   91    2    PD 0.1438\n71   91    3    PD 0.1464\n72   92    1    PD 0.2347\n73   92    2    PD 0.1677\n74   92    3    PD 0.1046\n75   93    1  CTRL 0.1657\n76   93    2  CTRL 0.1549\n77   93    3  CTRL 0.2212\n78   94    1    PD 0.1813\n79   94    2    PD 0.1235\n80   94    3    PD 0.1161\n81   96    1    PD 0.1249\n82   96    2    PD 0.1663\n83   97    1    PD 0.1575\n84   97    2    PD 0.1304\n85   97    3    PD 0.1579\n86   97    4    PD 0.1013\n87   98    1    PD 0.1980\n88   98    2    PD 0.2489\n89   98    3    PD 0.1549\n90   98    4    PD 0.0226\n91   99    1    PD 0.1383\n92   99    2    PD 0.1492\n93   99    3    PD 0.1687\n94   99    4    PD 0.1647\n95  101    1  CTRL 0.1089\n96  101    2  CTRL 0.1304\n97  101    3  CTRL 0.1174\n98  102    1    PD 0.2487\n99  102    2    PD 0.1620\n100 102    3    PD 0.1879\n101 102    4    PD 0.1719\n102 103    1    PD 0.2010\n103 103    2    PD 0.2284\n104 104    1    PD 0.1803\n105 104    2    PD 0.1699\n106 105    1    PD 0.2600\n107 105    2    PD 0.2527\n108 105    3    PD 0.2004\n109 106    1  CTRL 0.2083\n110 106    2  CTRL 0.2291\n111 106    3  CTRL 0.2996\n112 110    1    PD 0.2003\n113 110    2    PD 0.2601\n114 110    3    PD 0.2064\n115 112    1    PD 0.1480\n116 112    2    PD 0.1692\n117 112    3    PD 0.1173\n118 114    1  CTRL 0.1837\n119 114    2  CTRL 0.2439\n120 114    3  CTRL 0.2347\n121 115    1    PD 0.3499\n122 115    2    PD 0.1403\n123 115    3    PD 0.1079\n124 116    1    PD 0.2918\n125 116    2    PD 0.2381\n126 116    3    PD 0.1116\n127 120    1  CTRL 0.2246\n128 120    2  CTRL 0.3014\n129 122    1  CTRL 0.1899\n130 122    2  CTRL 0.2336\n131 122    3  CTRL 0.2829\n132 125    1    PD 0.1919\n133 125    2    PD 0.2513\n134 125    3    PD 0.2356\n135 127    1    PD 0.1732\n136 127    2    PD 0.1581\n137 127    3    PD 0.1552\n138 129    1    PD 0.1715\n139 129    2    PD 0.1550\n140 135    1    PD 0.2354\n141 135    2    PD 0.2128\n142 135    3    PD 0.1605\n143 136    1    PD 0.2952\n144 136    2    PD 0.3557\n145 136    3    PD 0.3414\n146 137    1    PD 0.2623\n147 137    2    PD 0.2815\n148 140    1    PD 0.2156\n149 140    2    PD 0.1495\n150 141    1  CTRL 0.2814\n151 141    2  CTRL 0.1975\n152 142    1  CTRL 0.2492\n153 142    2  CTRL 0.2348\n154 143    1    PD 0.3227\n155 143    2    PD 0.2401\n156 144    1  CTRL 0.2654\n157 144    2  CTRL 0.1684\n158 146    1    PD 0.2406\n159 146    2    PD 0.1947\n160 149    1  CTRL 0.3243\n161 149    2  CTRL 0.2345\n162 150    1  CTRL 0.1844\n163 150    2  CTRL 0.1644\n164 152    1    PD 0.2613\n165 152    2    PD 0.2944\n166 153    1  CTRL 0.2268\n167 153    2  CTRL 0.2117\n168 155    1    PD 0.1867\n169 155    2    PD 0.1588\n170 156    1    PD 0.2256\n171 156    2    PD 0.2887\n172 156    3    PD 0.2348\n173 159    1    PD 0.1554\n174 159    2    PD 0.0797\n175 160    1    PD 0.2001\n176 160    2    PD 0.2147\n177 162    1    PD 0.2374\n178 162    2    PD 0.2286\n179 162    3    PD 0.2794\n180 163    1  CTRL 0.1707\n181 163    2  CTRL 0.2192\n182 165    1    PD 0.2086\n183 165    2    PD 0.1367\n184 167    1    PD 0.3820\n185 167    2    PD 0.3491\n186 169    1  CTRL 0.2033\n187 169    2  CTRL 0.2925\n188 171    1    PD 0.2175\n189 171    2    PD 0.2671\n190 174    1    PD 0.2515\n191 174    2    PD 0.1912\n192 182    1    PD 0.1082\n193 182    2    PD 0.0860\n194 187    1    PD 0.0771\n195 187    2    PD 0.0665\n196 189    1    PD 0.1257\n197 189    2    PD 0.1837\n198 190    1    PD 0.1208\n199 190    2    PD 0.1008\n200 193    1    PD 0.2546\n201 193    2    PD 0.2372\n202 194    1  CTRL 0.2013\n203 194    2  CTRL 0.2298\n204 201    1    PD 0.1082\n205 201    2    PD 0.0919\n206 204    1    PD 0.1997\n207 204    2    PD 0.1913\n208 205    1  CTRL 0.1993\n209 205    2  CTRL 0.1017\n210 208    1    PD 0.0694\n211 208    2    PD 0.2629\n212 209    1    PD 0.1420\n213 209    2    PD 0.1933\n214 211    1    PD 0.1165\n215 211    2    PD 0.1632\n216 214    1    PD 0.1847\n217 214    2    PD 0.2078\n218 219    1  CTRL 0.2555\n219 219    2  CTRL 0.2970\n220 222    1    PD 0.2180\n221 222    2    PD 0.0967\n222 223    1    PD 0.2216\n223 223    2    PD 0.2817\n224 229    1    PD 0.1267\n225 229    2    PD 0.1553\n\npivot_wider\nThe pivot_wider() function takes two arguments: names_from which is the variable whose values will be converted to column names and values_from whose values will be cell values.\n\n# A tibble: 91 x 6\n      ID group    `1`    `2`    `3`   `4`\n   <int> <chr>  <dbl>  <dbl>  <dbl> <dbl>\n 1     6 PD    0.162  0.168   0.215    NA\n 2    29 PD    0.175  0.136  NA        NA\n 3    34 CTRL  0.166  0.140  NA        NA\n 4    36 CTRL  0.152  0.205  NA        NA\n 5    37 PD    0.219  0.158   0.259    NA\n 6    48 PD    0.130  0.270   0.248    NA\n 7    53 CTRL  0.211  0.152  NA        NA\n 8    54 PD    0.220  0.152   0.192    NA\n 9    58 PD    0.380  0.215   0.204    NA\n10    61 PD    0.0818 0.0628 NA        NA\n# … with 81 more rows\n\npivot_longer\nGoing back to long:\nThe pivot_longer function takes three arguments: cols is a list of columns that are to be collapsed. The columns can be referenced by column number or column name. names_to is the name of the new column which will combine all column names. This is up to you to decide what the name is. values_to is the name of the new column which will combine all column values associated with each variable combination.\n\n# A tibble: 364 x 4\n      ID group wave     DAN\n   <int> <chr> <chr>  <dbl>\n 1     6 PD    1      0.162\n 2     6 PD    2      0.168\n 3     6 PD    3      0.215\n 4     6 PD    4     NA    \n 5    29 PD    1      0.175\n 6    29 PD    2      0.136\n 7    29 PD    3     NA    \n 8    29 PD    4     NA    \n 9    34 CTRL  1      0.166\n10    34 CTRL  2      0.140\n# … with 354 more rows\n\nSeperate and Unite\nMany times datasets are, for a lack of a better term, messy. We will talk more about the upfront work later to make sure you dont have messy data. However, if you do have messy data there are a number of helpful functions to tidy-up your data.\nOne common way to represent longitudinal data is to name the variable with a wave signifier.\n\n# A tibble: 3 x 4\n     ID ext_1 ext_2 ext_3\n  <dbl> <dbl> <dbl> <dbl>\n1     1     4     4     4\n2     2     6     5     4\n3     3     4     5     6\n\nIf we went and tried to pivot_longer we’d end up with\n\n# A tibble: 9 x 3\n     ID time    EXT\n  <dbl> <chr> <dbl>\n1     1 ext_1     4\n2     1 ext_2     4\n3     1 ext_3     4\n4     2 ext_1     6\n5     2 ext_2     5\n6     2 ext_3     4\n7     3 ext_1     4\n8     3 ext_2     5\n9     3 ext_3     6\n\nThe time column is now specific to ext, which is a problem if I have more than one variable that I am pivoting. But, we will end up using wave as our time variable in our model, and time will have to be numeric. So how can we go ahead and separate out the ext part?\nOne way is to use the separate function\n\n# A tibble: 9 x 4\n     ID variable time    EXT\n  <dbl> <chr>    <chr> <dbl>\n1     1 ext      1         4\n2     1 ext      2         4\n3     1 ext      3         4\n4     2 ext      1         6\n5     2 ext      2         5\n6     2 ext      3         4\n7     3 ext      1         4\n8     3 ext      2         5\n9     3 ext      3         6\n\nIn terms of setting up your data, it is often helpful to include markers that separate parts of the variable eg \"_\" or “.” A variable that is ext_1 is easier to separate than ext1.\nNote, also that the time column is a character rather than numeric. We need to change this so as to use time continuously in our models. There are a few ways to do it, but this is perhaps the most straightforward.\n\n# A tibble: 9 x 4\n     ID variable  time   EXT\n  <dbl> <chr>    <dbl> <dbl>\n1     1 ext          1     4\n2     1 ext          2     4\n3     1 ext          3     4\n4     2 ext          1     6\n5     2 ext          2     5\n6     2 ext          3     4\n7     3 ext          1     4\n8     3 ext          2     5\n9     3 ext          3     6\n\nHowever, something that is a little more elegant is to do both the separating AND the making into numeric in the original pivot_longer function\nnames_prefix omits what is in there from the new cell names. Previously we had ext_1, ext_2, etc, which we had to seperate with a different function, but this does it within pivot_longer\n\n# A tibble: 9 x 3\n     ID time    EXT\n  <dbl> <chr> <dbl>\n1     1 1         4\n2     1 2         4\n3     1 3         4\n4     2 1         6\n5     2 2         5\n6     2 3         4\n7     3 1         4\n8     3 2         5\n9     3 3         6\n\nnames_transform does any transformations within the variables. Here instead of a separate call, we can make our variables numeric.\n\n# A tibble: 9 x 3\n     ID  time   EXT\n  <dbl> <dbl> <dbl>\n1     1     1     4\n2     1     2     4\n3     1     3     4\n4     2     1     6\n5     2     2     5\n6     2     3     4\n7     3     1     4\n8     3     2     5\n9     3     3     6\n\nAnother common problem that we often face is the need to unite two variables into one. Enter, the creatively titled unite function. Sometimes this happens when our time metric is entered in seperate columns.\n\n# A tibble: 3 x 6\n     ID  year month   day  hour   min\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1  2020     1     1     4    55\n2     2  2020     1     1     2    17\n3     3  2020     1     1     5    23\n\nTo combine them into one time metric\n\n# A tibble: 3 x 5\n     ID  year month   day time \n  <dbl> <dbl> <dbl> <dbl> <chr>\n1     1  2020     1     1 4:55 \n2     2  2020     1     1 2:17 \n3     3  2020     1     1 5:23 \n\ndate time metrics\n\n\n\nA date-time is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). These are called POSIXct in R.\n\n[1] \"2021-01-13\"\n\n\n[1] \"2021-01-13 13:45:43 CST\"\n\nBringing these into R from some outside place (excel, spss) can lead to confusion, as they can be formatted differently\n\n[1] \"2017-01-31\"\n[1] \"2017-01-31\"\n[1] \"2017-01-31\"\n\nYou can create these relatively straight forwardly…by hand\n\n[1] \"2017-01-31 20:11:59 UTC\"\n[1] \"2017-01-31 08:01:00 UTC\"\n\nOr you can use existing columns variables. This is where the lubridate package comes in handy\n\n# A tibble: 3 x 7\n     ID  year month   day  hour   min t_1                \n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dttm>             \n1     1  2020     1     1     4    55 2020-01-01 04:55:00\n2     2  2020     1     1     2    17 2020-01-01 02:17:00\n3     3  2020     1     1     5    23 2020-01-01 05:23:00\n\nNote the t_1 variable is a POSIXct variable type. Once in this format it is much easier to manipulate and work with dates and times.\nProjects and Rmarkdown\nAs with any project, but especially for longitudinal data, one of the most important aspects of data analysis is A. not losing track of what you did and B. being organized. This is much much much harder than said. I find using a combination of 1. rstudio projects 2. git and 3. codebooks are helpful in accomplishing these two goals. We will talk about #1 and #2 but I also encourage you to read about git. These are not the only way to do these sorts of analyses but I feel that exposure to them is helpful, as often in the social sciences these sort of decisions are not discussed.\nWhen I create an rmarkdown document for my own research projects, I always start by setting up 3 components:\nPackages\nCodebook(s)\nData\nBelow, we will step through each of these separately, setting ourselves up to (hopefully) flawlessly communicate with R and our data. Note that you do not need to use rmarkdown but I think rmarkdown is much more useful than standard .R syntax.\nPackages\nPackages seems like the most basic step, but it is actually very important. Depending on what gets loaded you might overwrite functions from other packages.(Note: I will often reload or not follow this advice within lectures for didactic reasons, choosing to put library calls above the code)\n\n\n\nCodebook\nThe second step is a codebook. Arguably, this is the first step because you should create the codebook long before you open R and load your data.\nWhy a codebook? Well, because you typically have a lot of variables and you will not be able to remember all the details that go into each one of them (rating scale, what the actual item was, was it coded someway, etc). This is especially true now that data are being collected online, which often provides placeholder variable names that then need to be processed somehow. This codebook will serve as a means to document RAW code. It will also allow us to automate some tasks that are somewhat cumbersome, facilitate open data practices, and efficiently see what variables are available. Ultimately, we want to be able to show how we got from the start, with the messy raw data, to our analyses and results at the end? A codebook makes this easier.\nTo illistrate, we are going to using some data from the German Socioeconomic Panel Study (GSOEP), which is an ongoing Panel Study in Germany. Note that these data are for teaching purposes only, shared under the license for the Comprehensive SOEP teaching dataset, which I, as a contracted SOEP user, can use for teaching purposes. These data represent select cases from the full data set and should not be used for the purpose of publication. The full data are available for free at https://www.diw.de/en/diw_02.c.222829.en/access_and_ordering.html.\nFor this tutorial, I created the codebook for you, and included what I believe are the core columns you may need. Some of these columns will not be particularly helpful for this dataset. For example, many of you likely work with datasets that have only a single file while others work with datasetsspread across many files (e.g., different waves, different sources). As a result, the “dataset” column of the codebook may only have a single value whereas for others it may have multiple.\nHere are my core columns that are based on the original data:\ndataset: this column indexes the name of the dataset that you will be pulling the data from. This is important because we will use this info later on (see purrr tutorial) to load and clean specific data files. Even if you don’t have multiple data sets, I believe consistency is more important and suggest using this.\nold_name: this column is the name of the variable in the data you are pulling it from. This should be exact. The goal of this column is that it will allow us to select() variables from the original data file and rename them something that is more useful to us. If you have worked with qualtrics (really any data) you know why this is important.\nitem_text: this column is the original text that participants saw or a description of the item.\nscale: this column tells you what the scale of the variable is. Is it a numeric variable, a text variable, etc. This is helpful for knowing the plausible range.\nreverse: this column tells you whether items in a scale need to be reverse coded. I recommend coding this as 1 (leave alone) and -1 (reverse) for reasons that will become clear later.\nmini: this column represents the minimum value of scales that are numeric. Leave blank otherwise.\nmaxi: this column represents the maximumv alue of scales that are numeric. Leave blank otherwise.\nrecode: sometimes, we want to recode variables for analyses (e.g. for categorical variables with many levels where sample sizes for some levels are too small to actually do anything with it). I use this column to note the kind of recoding I’ll do to a variable for transparency.\nHere are additional columns that will make our lives easier or are applicable to some but not all data sets:\ncategory: broad categories that different variables can be put into. I’m a fan of naming them things like “outcome”, “predictor”, “moderator”, “demographic”, “procedural”, etc. but sometimes use more descriptive labels like “Big 5” to indicate the model from which the measures are derived.\nlabel: label is basically one level lower than category. So if the category is Big 5, the label would be, or example, “A” for Agreeableness, “SWB” for subjective well-being, etc. This column is most important and useful when you have multiple items in a scales, so I’ll typically leave this blank when something is a standalone variable (e.g. sex, single-item scales, etc.).\nitem_name: This is the lowest level and most descriptive variable. It indicates which item in scale something is. So it may be “kind” for Agreebleness or “sex” for the demographic biological sex variable.\nyear: for longitudinal data, we have several waves of data and the name of the same item across waves is often different, so it’s important to note to which wave an item belongs. You can do this by noting the wave (e.g. 1, 2, 3), but I prefer the actual year the data were collected (e.g. 2005, 2009, etc.) if that is appropriate. See Lecture #1 on discussion of meaningful time metrics. Note that this differs from that discussion in your codebook you want to describe how you collected the data, not necessarily how you want to analyze the data.\nnew_name: This is a column that brings together much of the information we’ve already collected. It’s purpose is to be the new name that we will give to the variable that is more useful and descriptive to us. This is a constructed variable that brings together others. I like to make it a combination of “category”, “label”, “item_name”, and year using varying combos of \"_\" and “.” that we can use later with tidyverse functions. I typically construct this variable in Excel using the CONCATENATE() function, but it could also be done in R. The reason I do it in Excel is that it makes it easier for someone who may be reviewing my codebook.\nThere is a separate discussion to be had on naming conventions for your variables, but the important idea to remember is that names convey important information and we want to use this information later on to make our life easier. By coding these variables using this information AND systematically using different separators we can accomplish this goal.\nmeta: Some datasets have a meta name, which essentially means a name that variable has across all waves to make it clear which variables are the same. They are not always useful as some data sets have meta names but no great way of extracting variables using them. But they’re still typically useful to include in your codebook regardless.\nThese are just suggestions, but after working with many longitudinal datasets I will say all of them are horrible in some way. Doing this makes them less horrible. Is it some upfront work? Yes. Will it ultimately save you time? Yes. Also, if you know this prior to runnign a study you are making some sort of code book anyways, right, right? Might as well kill two birds with one stone.\nYou can make the codebook anyway you want, but the two best options are miscrosoft excel and google pages. Not because they are necessarily the best functioning but because they are relatively ubiquitous and are easy to share.\nWe will create a codebook but then bring the codebook into R via turning it into a csv. You can rethink the codebook as a way of coding prior to putting anything into R.\nBelow, I’ll load in the codebook we will use for this study, which will include all of the above columns.\n\n# A tibble: 153 x 13\n   dataset old_name item_text scale category label item_name  year\n   <chr>   <chr>    <chr>     <chr> <chr>    <chr> <chr>     <dbl>\n 1 <NA>    persnr   Never Ch…  <NA> Procedu… <NA>  SID           0\n 2 <NA>    hhnr     househol…  <NA> Procedu… <NA>  household     0\n 3 ppfad   gebjahr  Year of … \"num… Demogra… <NA>  DOB           0\n 4 ppfad   sex      Sex       \"\\n1… Demogra… <NA>  Sex           0\n 5 vp      vp12501  Thorough…  <NA> Big 5    C     thorough   2005\n 6 zp      zp12001  Thorough…  <NA> Big 5    C     thorough   2009\n 7 bdp     bdp15101 Thorough…  <NA> Big 5    C     thorough   2013\n 8 vp      vp12502  Am commu…  <NA> Big 5    E     communic   2005\n 9 zp      zp12002  Am commu…  <NA> Big 5    E     communic   2009\n10 bdp     bdp15102 Am commu…  <NA> Big 5    E     communic   2013\n# … with 143 more rows, and 5 more variables: new_name <chr>,\n#   reverse <dbl>, mini <dbl>, maxi <dbl>, recode <chr>\n\nData\nFirst, we need to load in the data. We’re going to use three waves of data from the German Socioeconomic Panel Study, which is a longitudinal study of German households that has been conducted since 1984. We’re going to use more recent data from three waves of personality data collected between 2005 and 2013.\nNote: we will be using the teaching set of the GSOEP data set. I will not be pulling from the raw files as a result of this. I will also not be mirroring the format that you would usually load the GSOEP from because that is slightly more complicated and somethng we will return to in a later tutorial after we have more skills. I’ve left that code for now, but it won’t make a lot of sense right now.\nThis code below shows how I would read in and rename a wide-format data set using the codebook I created.\n\n# A tibble: 28,290 x 153\n   Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex\n             <dbl>            <dbl>            <dbl>            <dbl>\n 1             901               94             1951                2\n 2            1202              124             1913                2\n 3            2301              230             1946                1\n 4            2302              230             1946                2\n 5            2304              230             1978                1\n 6            2305              230             1946                2\n 7            4601              469             1933                2\n 8            4701              477             1919                2\n 9            4901              493             1925                2\n10            5201              523             1955                1\n# … with 28,280 more rows, and 149 more variables: `Big\n#   5__C_thorough.2005` <dbl>, `Big 5__C_thorough.2009` <dbl>, `Big\n#   5__C_thorough.2013` <dbl>, `Big 5__E_communic.2005` <dbl>, `Big\n#   5__E_communic.2009` <dbl>, `Big 5__E_communic.2013` <dbl>, `Big\n#   5__A_coarse.2005` <dbl>, `Big 5__A_coarse.2009` <dbl>, `Big\n#   5__A_coarse.2013` <dbl>, `Big 5__O_original.2005` <dbl>, `Big\n#   5__O_original.2009` <dbl>, `Big 5__O_original.2013` <dbl>, `Big\n#   5__N_worry.2005` <dbl>, `Big 5__N_worry.2009` <dbl>, `Big\n#   5__N_worry.2013` <dbl>, `Big 5__A_forgive.2005` <dbl>, `Big\n#   5__A_forgive.2009` <dbl>, `Big 5__A_forgive.2013` <dbl>, `Big\n#   5__C_lazy.2005` <dbl>, `Big 5__C_lazy.2009` <dbl>, `Big\n#   5__C_lazy.2013` <dbl>, `Big 5__E_sociable.2005` <dbl>, `Big\n#   5__E_sociable.2009` <dbl>, `Big 5__E_sociable.2013` <dbl>, `Big\n#   5__O_artistic.2005` <dbl>, `Big 5__O_artistic.2009` <dbl>, `Big\n#   5__O_artistic.2013` <dbl>, `Big 5__N_nervous.2005` <dbl>, `Big\n#   5__N_nervous.2009` <dbl>, `Big 5__N_nervous.2013` <dbl>, `Big\n#   5__C_efficient.2005` <dbl>, `Big 5__C_efficient.2009` <dbl>, `Big\n#   5__C_efficient.2013` <dbl>, `Big 5__E_reserved.2005` <dbl>, `Big\n#   5__E_reserved.2009` <dbl>, `Big 5__E_reserved.2013` <dbl>, `Big\n#   5__A_friendly.2005` <dbl>, `Big 5__A_friendly.2009` <dbl>, `Big\n#   5__A_friendly.2013` <dbl>, `Big 5__O_imagin.2005` <dbl>, `Big\n#   5__O_imagin.2009` <dbl>, `Big 5__O_imagin.2013` <dbl>, `Big\n#   5__N_dealStress.2005` <dbl>, `Big 5__N_dealStress.2009` <dbl>,\n#   `Big 5__N_dealStress.2013` <dbl>, `Life\n#   Event__ChldBrth.2005` <dbl>, `Life Event__ChldBrth.2006` <dbl>,\n#   `Life Event__ChldBrth.2007` <dbl>, `Life\n#   Event__ChldBrth.2008` <dbl>, `Life Event__ChldBrth.2009` <dbl>,\n#   `Life Event__ChldBrth.2010` <dbl>, `Life\n#   Event__ChldBrth.2011` <dbl>, `Life Event__ChldBrth.2012` <dbl>,\n#   `Life Event__ChldBrth.2013` <dbl>, `Life\n#   Event__ChldBrth.2014` <dbl>, `Life Event__ChldBrth.2015` <dbl>,\n#   `Life Event__ChldMvOut.2005` <dbl>, `Life\n#   Event__ChldMvOut.2006` <dbl>, `Life Event__ChldMvOut.2007` <dbl>,\n#   `Life Event__ChldMvOut.2008` <dbl>, `Life\n#   Event__ChldMvOut.2009` <dbl>, `Life Event__ChldMvOut.2010` <dbl>,\n#   `Life Event__ChldMvOut.2011` <dbl>, `Life\n#   Event__ChldMvOut.2012` <dbl>, `Life Event__ChldMvOut.2013` <dbl>,\n#   `Life Event__ChldMvOut.2014` <dbl>, `Life\n#   Event__ChldMvOut.2015` <dbl>, `Life Event__Divorce.2005` <dbl>,\n#   `Life Event__Divorce.2006` <dbl>, `Life\n#   Event__Divorce.2007` <dbl>, `Life Event__Divorce.2008` <dbl>,\n#   `Life Event__Divorce.2009` <dbl>, `Life\n#   Event__Divorce.2010` <dbl>, `Life Event__Divorce.2011` <dbl>,\n#   `Life Event__Divorce.2012` <dbl>, `Life\n#   Event__Divorce.2013` <dbl>, `Life Event__Divorce.2014` <dbl>,\n#   `Life Event__Divorce.2015` <dbl>, `Life\n#   Event__DadDied.2005` <dbl>, `Life Event__DadDied.2006` <dbl>,\n#   `Life Event__DadDied.2007` <dbl>, `Life\n#   Event__DadDied.2008` <dbl>, `Life Event__DadDied.2009` <dbl>,\n#   `Life Event__DadDied.2010` <dbl>, `Life\n#   Event__DadDied.2011` <dbl>, `Life Event__DadDied.2012` <dbl>,\n#   `Life Event__DadDied.2013` <dbl>, `Life\n#   Event__DadDied.2014` <dbl>, `Life Event__DadDied.2015` <dbl>,\n#   `Life Event__NewPart.2011` <dbl>, `Life\n#   Event__NewPart.2012` <dbl>, `Life Event__NewPart.2013` <dbl>,\n#   `Life Event__NewPart.2014` <dbl>, `Life\n#   Event__NewPart.2015` <dbl>, `Life Event__Married.2005` <dbl>,\n#   `Life Event__Married.2006` <dbl>, `Life\n#   Event__Married.2007` <dbl>, `Life Event__Married.2008` <dbl>,\n#   `Life Event__Married.2009` <dbl>, `Life\n#   Event__Married.2010` <dbl>, …\n\nClean Data\nRecode Variables\nMany of the data we work with have observations that are missing for a variety of reasons. In R, we treat missing values as NA, but many other programs from which you may be importing your data may use other codes (e.g. 999, -999, etc.). Large panel studies tend to use small negative values to indicate different types of missingness. This is why it is important to note down the scale in your codebook. That way you can check which values may need to be recoded to explicit NA values.\nIn the GSOEP, -1 to -7 indicate various types of missing values, so we will recode these to NA. To do this, we will use mapvalues(), from the plyr package. In later tutorials where we read in and manipulate more complex data sets, we will use mapvalues() a lot.\nBelow we are taking the dataset soep and saying we are going to mutate all of our variables, making sure they are all numeric (as the code expects numeric), then we get to the mapvalues function:\nmapvalues takes 3 key arguments: (1) the variable you are recoding. Below that is indicated by “.” which is shorthand for the data that was piped in.\na vector of initial values from which you want to change. Here we indicae a sequence of values from -1 to -7, which correspond to the missing values used by GSOEP. Other datasets may use -999, for example.\nrecode your values in (2) to new values in the same order as the old values. Here we have NA (the way R treats missing data) repeated 7 times (to correspond to -1, -2,…)\nIt is also helpful to turn off warnings if some levels are not in your data (warn_missing = F).\n\n# A tibble: 28,290 x 153\n   Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex\n             <dbl>            <dbl>            <dbl>            <dbl>\n 1             901               94             1951                2\n 2            1202              124             1913                2\n 3            2301              230             1946                1\n 4            2302              230             1946                2\n 5            2304              230             1978                1\n 6            2305              230             1946                2\n 7            4601              469             1933                2\n 8            4701              477             1919                2\n 9            4901              493             1925                2\n10            5201              523             1955                1\n# … with 28,280 more rows, and 149 more variables: `Big\n#   5__C_thorough.2005` <dbl>, `Big 5__C_thorough.2009` <dbl>, `Big\n#   5__C_thorough.2013` <dbl>, `Big 5__E_communic.2005` <dbl>, `Big\n#   5__E_communic.2009` <dbl>, `Big 5__E_communic.2013` <dbl>, `Big\n#   5__A_coarse.2005` <dbl>, `Big 5__A_coarse.2009` <dbl>, `Big\n#   5__A_coarse.2013` <dbl>, `Big 5__O_original.2005` <dbl>, `Big\n#   5__O_original.2009` <dbl>, `Big 5__O_original.2013` <dbl>, `Big\n#   5__N_worry.2005` <dbl>, `Big 5__N_worry.2009` <dbl>, `Big\n#   5__N_worry.2013` <dbl>, `Big 5__A_forgive.2005` <dbl>, `Big\n#   5__A_forgive.2009` <dbl>, `Big 5__A_forgive.2013` <dbl>, `Big\n#   5__C_lazy.2005` <dbl>, `Big 5__C_lazy.2009` <dbl>, `Big\n#   5__C_lazy.2013` <dbl>, `Big 5__E_sociable.2005` <dbl>, `Big\n#   5__E_sociable.2009` <dbl>, `Big 5__E_sociable.2013` <dbl>, `Big\n#   5__O_artistic.2005` <dbl>, `Big 5__O_artistic.2009` <dbl>, `Big\n#   5__O_artistic.2013` <dbl>, `Big 5__N_nervous.2005` <dbl>, `Big\n#   5__N_nervous.2009` <dbl>, `Big 5__N_nervous.2013` <dbl>, `Big\n#   5__C_efficient.2005` <dbl>, `Big 5__C_efficient.2009` <dbl>, `Big\n#   5__C_efficient.2013` <dbl>, `Big 5__E_reserved.2005` <dbl>, `Big\n#   5__E_reserved.2009` <dbl>, `Big 5__E_reserved.2013` <dbl>, `Big\n#   5__A_friendly.2005` <dbl>, `Big 5__A_friendly.2009` <dbl>, `Big\n#   5__A_friendly.2013` <dbl>, `Big 5__O_imagin.2005` <dbl>, `Big\n#   5__O_imagin.2009` <dbl>, `Big 5__O_imagin.2013` <dbl>, `Big\n#   5__N_dealStress.2005` <dbl>, `Big 5__N_dealStress.2009` <dbl>,\n#   `Big 5__N_dealStress.2013` <dbl>, `Life\n#   Event__ChldBrth.2005` <dbl>, `Life Event__ChldBrth.2006` <dbl>,\n#   `Life Event__ChldBrth.2007` <dbl>, `Life\n#   Event__ChldBrth.2008` <dbl>, `Life Event__ChldBrth.2009` <dbl>,\n#   `Life Event__ChldBrth.2010` <dbl>, `Life\n#   Event__ChldBrth.2011` <dbl>, `Life Event__ChldBrth.2012` <dbl>,\n#   `Life Event__ChldBrth.2013` <dbl>, `Life\n#   Event__ChldBrth.2014` <dbl>, `Life Event__ChldBrth.2015` <dbl>,\n#   `Life Event__ChldMvOut.2005` <dbl>, `Life\n#   Event__ChldMvOut.2006` <dbl>, `Life Event__ChldMvOut.2007` <dbl>,\n#   `Life Event__ChldMvOut.2008` <dbl>, `Life\n#   Event__ChldMvOut.2009` <dbl>, `Life Event__ChldMvOut.2010` <dbl>,\n#   `Life Event__ChldMvOut.2011` <dbl>, `Life\n#   Event__ChldMvOut.2012` <dbl>, `Life Event__ChldMvOut.2013` <dbl>,\n#   `Life Event__ChldMvOut.2014` <dbl>, `Life\n#   Event__ChldMvOut.2015` <dbl>, `Life Event__Divorce.2005` <dbl>,\n#   `Life Event__Divorce.2006` <dbl>, `Life\n#   Event__Divorce.2007` <dbl>, `Life Event__Divorce.2008` <dbl>,\n#   `Life Event__Divorce.2009` <dbl>, `Life\n#   Event__Divorce.2010` <dbl>, `Life Event__Divorce.2011` <dbl>,\n#   `Life Event__Divorce.2012` <dbl>, `Life\n#   Event__Divorce.2013` <dbl>, `Life Event__Divorce.2014` <dbl>,\n#   `Life Event__Divorce.2015` <dbl>, `Life\n#   Event__DadDied.2005` <dbl>, `Life Event__DadDied.2006` <dbl>,\n#   `Life Event__DadDied.2007` <dbl>, `Life\n#   Event__DadDied.2008` <dbl>, `Life Event__DadDied.2009` <dbl>,\n#   `Life Event__DadDied.2010` <dbl>, `Life\n#   Event__DadDied.2011` <dbl>, `Life Event__DadDied.2012` <dbl>,\n#   `Life Event__DadDied.2013` <dbl>, `Life\n#   Event__DadDied.2014` <dbl>, `Life Event__DadDied.2015` <dbl>,\n#   `Life Event__NewPart.2011` <dbl>, `Life\n#   Event__NewPart.2012` <dbl>, `Life Event__NewPart.2013` <dbl>,\n#   `Life Event__NewPart.2014` <dbl>, `Life\n#   Event__NewPart.2015` <dbl>, `Life Event__Married.2005` <dbl>,\n#   `Life Event__Married.2006` <dbl>, `Life\n#   Event__Married.2007` <dbl>, `Life Event__Married.2008` <dbl>,\n#   `Life Event__Married.2009` <dbl>, `Life\n#   Event__Married.2010` <dbl>, …\n\nReverse-Scoring\nMany scales we use have items that are positively or negatively keyed. High ratings on positively keyed items are indicative of being high on a construct. In contrast, high ratings on negatively keyed items are indicative of being low on a construct. Thus, to create the composite scores of constructs we often use, we must first “reverse” the negatively keyed items so that high scores indicate being higher on the construct.\nThere are a few ways to do this in R. Below, I’ll demonstrate how to do so using the reverse.code() function in the psych package in R. This function was built to make reverse coding more efficient (i.e. please don’t run every item that needs to be recoded with separate lines of code!!).\nBefore we can do that, though, we need to restructure the data a bit in order to bring in the reverse coding information from our codebook.\n\n# A tibble: 28,290 x 153\n   Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex\n             <dbl>            <dbl>            <dbl>            <dbl>\n 1             901               94             1951                2\n 2            1202              124             1913                2\n 3            2301              230             1946                1\n 4            2302              230             1946                2\n 5            2304              230             1978                1\n 6            2305              230             1946                2\n 7            4601              469             1933                2\n 8            4701              477             1919                2\n 9            4901              493             1925                2\n10            5201              523             1955                1\n# … with 28,280 more rows, and 149 more variables: `Big\n#   5__C_thorough.2005` <dbl>, `Big 5__C_thorough.2009` <dbl>, `Big\n#   5__C_thorough.2013` <dbl>, `Big 5__E_communic.2005` <dbl>, `Big\n#   5__E_communic.2009` <dbl>, `Big 5__E_communic.2013` <dbl>, `Big\n#   5__A_coarse.2005` <dbl>, `Big 5__A_coarse.2009` <dbl>, `Big\n#   5__A_coarse.2013` <dbl>, `Big 5__O_original.2005` <dbl>, `Big\n#   5__O_original.2009` <dbl>, `Big 5__O_original.2013` <dbl>, `Big\n#   5__N_worry.2005` <dbl>, `Big 5__N_worry.2009` <dbl>, `Big\n#   5__N_worry.2013` <dbl>, `Big 5__A_forgive.2005` <dbl>, `Big\n#   5__A_forgive.2009` <dbl>, `Big 5__A_forgive.2013` <dbl>, `Big\n#   5__C_lazy.2005` <dbl>, `Big 5__C_lazy.2009` <dbl>, `Big\n#   5__C_lazy.2013` <dbl>, `Big 5__E_sociable.2005` <dbl>, `Big\n#   5__E_sociable.2009` <dbl>, `Big 5__E_sociable.2013` <dbl>, `Big\n#   5__O_artistic.2005` <dbl>, `Big 5__O_artistic.2009` <dbl>, `Big\n#   5__O_artistic.2013` <dbl>, `Big 5__N_nervous.2005` <dbl>, `Big\n#   5__N_nervous.2009` <dbl>, `Big 5__N_nervous.2013` <dbl>, `Big\n#   5__C_efficient.2005` <dbl>, `Big 5__C_efficient.2009` <dbl>, `Big\n#   5__C_efficient.2013` <dbl>, `Big 5__E_reserved.2005` <dbl>, `Big\n#   5__E_reserved.2009` <dbl>, `Big 5__E_reserved.2013` <dbl>, `Big\n#   5__A_friendly.2005` <dbl>, `Big 5__A_friendly.2009` <dbl>, `Big\n#   5__A_friendly.2013` <dbl>, `Big 5__O_imagin.2005` <dbl>, `Big\n#   5__O_imagin.2009` <dbl>, `Big 5__O_imagin.2013` <dbl>, `Big\n#   5__N_dealStress.2005` <dbl>, `Big 5__N_dealStress.2009` <dbl>,\n#   `Big 5__N_dealStress.2013` <dbl>, `Life\n#   Event__ChldBrth.2005` <dbl>, `Life Event__ChldBrth.2006` <dbl>,\n#   `Life Event__ChldBrth.2007` <dbl>, `Life\n#   Event__ChldBrth.2008` <dbl>, `Life Event__ChldBrth.2009` <dbl>,\n#   `Life Event__ChldBrth.2010` <dbl>, `Life\n#   Event__ChldBrth.2011` <dbl>, `Life Event__ChldBrth.2012` <dbl>,\n#   `Life Event__ChldBrth.2013` <dbl>, `Life\n#   Event__ChldBrth.2014` <dbl>, `Life Event__ChldBrth.2015` <dbl>,\n#   `Life Event__ChldMvOut.2005` <dbl>, `Life\n#   Event__ChldMvOut.2006` <dbl>, `Life Event__ChldMvOut.2007` <dbl>,\n#   `Life Event__ChldMvOut.2008` <dbl>, `Life\n#   Event__ChldMvOut.2009` <dbl>, `Life Event__ChldMvOut.2010` <dbl>,\n#   `Life Event__ChldMvOut.2011` <dbl>, `Life\n#   Event__ChldMvOut.2012` <dbl>, `Life Event__ChldMvOut.2013` <dbl>,\n#   `Life Event__ChldMvOut.2014` <dbl>, `Life\n#   Event__ChldMvOut.2015` <dbl>, `Life Event__Divorce.2005` <dbl>,\n#   `Life Event__Divorce.2006` <dbl>, `Life\n#   Event__Divorce.2007` <dbl>, `Life Event__Divorce.2008` <dbl>,\n#   `Life Event__Divorce.2009` <dbl>, `Life\n#   Event__Divorce.2010` <dbl>, `Life Event__Divorce.2011` <dbl>,\n#   `Life Event__Divorce.2012` <dbl>, `Life\n#   Event__Divorce.2013` <dbl>, `Life Event__Divorce.2014` <dbl>,\n#   `Life Event__Divorce.2015` <dbl>, `Life\n#   Event__DadDied.2005` <dbl>, `Life Event__DadDied.2006` <dbl>,\n#   `Life Event__DadDied.2007` <dbl>, `Life\n#   Event__DadDied.2008` <dbl>, `Life Event__DadDied.2009` <dbl>,\n#   `Life Event__DadDied.2010` <dbl>, `Life\n#   Event__DadDied.2011` <dbl>, `Life Event__DadDied.2012` <dbl>,\n#   `Life Event__DadDied.2013` <dbl>, `Life\n#   Event__DadDied.2014` <dbl>, `Life Event__DadDied.2015` <dbl>,\n#   `Life Event__NewPart.2011` <dbl>, `Life\n#   Event__NewPart.2012` <dbl>, `Life Event__NewPart.2013` <dbl>,\n#   `Life Event__NewPart.2014` <dbl>, `Life\n#   Event__NewPart.2015` <dbl>, `Life Event__Married.2005` <dbl>,\n#   `Life Event__Married.2006` <dbl>, `Life\n#   Event__Married.2007` <dbl>, `Life Event__Married.2008` <dbl>,\n#   `Life Event__Married.2009` <dbl>, `Life\n#   Event__Married.2010` <dbl>, …\n\nBring the wide dataset to long\n\n# A tibble: 471,722 x 6\n   Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex\n             <dbl>            <dbl>            <dbl>            <dbl>\n 1             901               94             1951                2\n 2             901               94             1951                2\n 3             901               94             1951                2\n 4             901               94             1951                2\n 5             901               94             1951                2\n 6             901               94             1951                2\n 7             901               94             1951                2\n 8             901               94             1951                2\n 9             901               94             1951                2\n10             901               94             1951                2\n# … with 471,712 more rows, and 2 more variables: item <chr>,\n#   value <dbl>\n\nBring in the codebook relevent items for reverse coding\n\n# A tibble: 471,722 x 9\n   Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex\n             <dbl>            <dbl>            <dbl>            <dbl>\n 1             901               94             1951                2\n 2             901               94             1951                2\n 3             901               94             1951                2\n 4             901               94             1951                2\n 5             901               94             1951                2\n 6             901               94             1951                2\n 7             901               94             1951                2\n 8             901               94             1951                2\n 9             901               94             1951                2\n10             901               94             1951                2\n# … with 471,712 more rows, and 5 more variables: item <chr>,\n#   value <dbl>, reverse <dbl>, mini <dbl>, maxi <dbl>\n\nHere we want to break our item column up into different components to assist with different calculations. Often you will have some sort of heirachy of variables where items are nested within scales which are nested within questionnaires. This is where you can code that information.\n\n# A tibble: 471,722 x 12\n   Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex\n             <dbl>            <dbl>            <dbl>            <dbl>\n 1             901               94             1951                2\n 2             901               94             1951                2\n 3             901               94             1951                2\n 4             901               94             1951                2\n 5             901               94             1951                2\n 6             901               94             1951                2\n 7             901               94             1951                2\n 8             901               94             1951                2\n 9             901               94             1951                2\n10             901               94             1951                2\n# … with 471,712 more rows, and 8 more variables: type <chr>,\n#   trait <chr>, item <chr>, year <chr>, value <dbl>, reverse <dbl>,\n#   mini <dbl>, maxi <dbl>\n\nnow it is ready to reverse code!\n\n# A tibble: 471,722 x 12\n   Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex\n             <dbl>            <dbl>            <dbl>            <dbl>\n 1             901               94             1951                2\n 2             901               94             1951                2\n 3             901               94             1951                2\n 4             901               94             1951                2\n 5             901               94             1951                2\n 6             901               94             1951                2\n 7             901               94             1951                2\n 8             901               94             1951                2\n 9             901               94             1951                2\n10             901               94             1951                2\n# … with 471,712 more rows, and 8 more variables: type <chr>,\n#   trait <chr>, item <chr>, year <chr>, value <dbl>, reverse <dbl>,\n#   mini <dbl>, maxi <dbl>\n\nCreate Composites\nNow that we have reverse coded our items, we can create composites.\nBFI-S\nWe’ll start with our scale – in this case, the Big 5 from the German translation of the BFI-S.\nHere’s the simplest way, which is also the long way because you’d have to do it for each scale, in each year, which I don’t recommend as that will be MANY MANY lines of code.\n\n\n\nWe can use our codebook and dplyr to make our lives a whole lot easier. In general, trying to run everything simultanously saves from copy-paste errors, makes your code more readable, and reduces the total amount of code. So while the below code may not make intuiative sense immediately, it is nonetheless what we are working towards. Also, going through line by line will help you see that\nFirst, make sure we are only working with Big Five rows. Notice how once we filter the row size decreases.\n\n# A tibble: 452,104 x 12\n   Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex\n             <dbl>            <dbl>            <dbl>            <dbl>\n 1             901               94             1951                2\n 2             901               94             1951                2\n 3             901               94             1951                2\n 4             901               94             1951                2\n 5             901               94             1951                2\n 6             901               94             1951                2\n 7             901               94             1951                2\n 8             901               94             1951                2\n 9             901               94             1951                2\n10             901               94             1951                2\n# … with 452,094 more rows, and 8 more variables: type <chr>,\n#   trait <chr>, item <chr>, year <chr>, value <dbl>, reverse <dbl>,\n#   mini <dbl>, maxi <dbl>\n\n\n# A tibble: 151,186 x 4\n# Groups:   Procedural__SID, trait [83,572]\n   Procedural__SID trait year  value\n             <dbl> <chr> <chr> <dbl>\n 1             901 A     2005   4.67\n 2             901 A     2009   5   \n 3             901 A     2013   4.67\n 4             901 C     2005   5   \n 5             901 C     2009   5   \n 6             901 C     2013   5.67\n 7             901 E     2005   3.67\n 8             901 E     2009   3.67\n 9             901 E     2013   3.67\n10             901 N     2005   3.33\n# … with 151,176 more rows\n\nNow that we have our means we can bring the demographic info back into the dataframe…or whatever else you would want to bring in.\n\n# A tibble: 151,186 x 6\n   Procedural__SID trait year  value   DOB   Sex\n             <dbl> <chr> <chr> <dbl> <dbl> <dbl>\n 1             901 A     2005   4.67  1951     2\n 2             901 A     2009   5     1951     2\n 3             901 A     2013   4.67  1951     2\n 4             901 C     2005   5     1951     2\n 5             901 C     2009   5     1951     2\n 6             901 C     2013   5.67  1951     2\n 7             901 E     2005   3.67  1951     2\n 8             901 E     2009   3.67  1951     2\n 9             901 E     2013   3.67  1951     2\n10             901 N     2005   3.33  1951     2\n# … with 151,176 more rows\n\nDescriptives\nDescriptives of your data are incredibly important. They are your first line of defense against things that could go wrong later on when you run inferential stats. They help you check the distribution of your variables (e.g. non-normally distributed), look for implausible values made through coding or participant error, and allow you to anticipate what your findings will look like.\nThere are lots of ways to create great tables of descriptives. My favorite way is using dplyr, but we will save that for a later lesson on creating great APA style tables in R. For now, we’ll use a wonderfully helpful function from the psych package called describe() in conjunction with a small amount of tidyr to reshape the data.\nmetric variables\n\n# A tibble: 151,186 x 6\n   Procedural__SID trait year  value   DOB   Sex\n             <dbl> <chr> <chr> <dbl> <dbl> <dbl>\n 1             901 A     2005   4.67  1951     2\n 2             901 A     2009   5     1951     2\n 3             901 A     2013   4.67  1951     2\n 4             901 C     2005   5     1951     2\n 5             901 C     2009   5     1951     2\n 6             901 C     2013   5.67  1951     2\n 7             901 E     2005   3.67  1951     2\n 8             901 E     2009   3.67  1951     2\n 9             901 E     2013   3.67  1951     2\n10             901 N     2005   3.33  1951     2\n# … with 151,176 more rows\n\n\n# A tibble: 151,186 x 5\n   Procedural__SID tmp    value   DOB   Sex\n             <dbl> <chr>  <dbl> <dbl> <dbl>\n 1             901 A_2005  4.67  1951     2\n 2             901 A_2009  5     1951     2\n 3             901 A_2013  4.67  1951     2\n 4             901 C_2005  5     1951     2\n 5             901 C_2009  5     1951     2\n 6             901 C_2013  5.67  1951     2\n 7             901 E_2005  3.67  1951     2\n 8             901 E_2009  3.67  1951     2\n 9             901 E_2013  3.67  1951     2\n10             901 N_2005  3.33  1951     2\n# … with 151,176 more rows\n\n\n# A tibble: 16,719 x 18\n   Procedural__SID   DOB   Sex A_2005 A_2009 A_2013 C_2005 C_2009\n             <dbl> <dbl> <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1             901  1951     2   4.67   5      4.67   5      5   \n 2            1202  1913     2   7     NA     NA      6     NA   \n 3            2301  1946     1   5      5.67   6      5.67   6.33\n 4            2302  1946     2   3.33   3.33   2.67   6      6   \n 5            2304  1978     1   5.67  NA     NA      4.33  NA   \n 6            4601  1933     2   6.33  NA     NA      6.5   NA   \n 7            4701  1919     2   6      5.67  NA      6.33   5.67\n 8            4901  1925     2   5.33   4      4.67   5.33   6   \n 9            5201  1955     1   6.33   4.67   5.33   7      5   \n10            5202  1956     2   5      5      6      5.67   5.33\n# … with 16,709 more rows, and 10 more variables: C_2013 <dbl>,\n#   E_2005 <dbl>, E_2009 <dbl>, E_2013 <dbl>, N_2005 <dbl>,\n#   N_2009 <dbl>, N_2013 <dbl>, O_2005 <dbl>, O_2009 <dbl>,\n#   O_2013 <dbl>\n\n\n                vars     n       mean          sd     median\nProcedural__SID    1 16719 8321022.91 10677731.19 3105002.00\nDOB                2 16719    1960.03       18.48    1960.00\nSex                3 16719       1.52        0.50       2.00\nA_2005             4 10419       5.46        0.98       5.67\nA_2009             5 10294       5.35        0.99       5.33\nA_2013             6  9535       5.41        0.95       5.33\nC_2005             7 10412       5.90        0.96       6.00\nC_2009             8 10290       5.82        0.95       6.00\nC_2013             9  9530       5.84        0.91       6.00\nE_2005            10 10416       4.82        1.15       5.00\nE_2009            11 10291       4.77        1.15       4.67\nE_2013            12  9533       4.87        1.11       5.00\nN_2005            13 10413       4.05        1.23       4.00\nN_2009            14 10294       4.18        1.22       4.33\nN_2013            15  9534       4.25        1.21       4.33\nO_2005            16 10408       4.51        1.22       4.67\nO_2009            17 10287       4.40        1.22       4.33\nO_2013            18  9530       4.60        1.18       4.67\n                   trimmed        mad  min      max    range  skew\nProcedural__SID 6487615.30 3735541.17  901 35022002 35021101  1.50\nDOB                1960.22      20.76 1909     1995       86 -0.08\nSex                   1.53       0.00    1        2        1 -0.10\nA_2005                5.50       0.99    1        7        6 -0.40\nA_2009                5.38       0.99    1        7        6 -0.37\nA_2013                5.45       0.99    1        7        6 -0.42\nC_2005                6.01       0.99    1        7        6 -0.96\nC_2009                5.92       0.99    1        7        6 -0.82\nC_2013                5.92       0.99    1        7        6 -0.72\nE_2005                4.85       0.99    1        7        6 -0.27\nE_2009                4.80       0.99    1        7        6 -0.24\nE_2013                4.90       0.99    1        7        6 -0.29\nN_2005                4.06       1.48    1        7        6 -0.07\nN_2009                4.20       0.99    1        7        6 -0.17\nN_2013                4.27       1.24    1        7        6 -0.15\nO_2005                4.53       1.48    1        7        6 -0.23\nO_2009                4.41       1.48    1        7        6 -0.10\nO_2013                4.62       0.99    1        7        6 -0.21\n                kurtosis       se\nProcedural__SID     0.56 82579.80\nDOB                -0.84     0.14\nSex                -1.99     0.00\nA_2005             -0.16     0.01\nA_2009             -0.19     0.01\nA_2013              0.04     0.01\nC_2005              0.87     0.01\nC_2009              0.49     0.01\nC_2013              0.18     0.01\nE_2005             -0.16     0.01\nE_2009             -0.17     0.01\nE_2013             -0.18     0.01\nN_2005             -0.32     0.01\nN_2009             -0.30     0.01\nN_2013             -0.30     0.01\nO_2005             -0.18     0.01\nO_2009             -0.30     0.01\nO_2013             -0.20     0.01\n\ncount variables\nWe have life event variable in the dataset that is a count variable. It asks did someone experience a life event during the previous year. also want to create a variable that indexes whether our participants experienced any of the life events during the years of interest (2005-2015).\n# A tibble: 19,618 x 12\n   Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex\n             <dbl>            <dbl>            <dbl>            <dbl>\n 1             901               94             1951                2\n 2             901               94             1951                2\n 3            2301              230             1946                1\n 4            2301              230             1946                1\n 5            2301              230             1946                1\n 6            2305              230             1946                2\n 7            4601              469             1933                2\n 8            5201              523             1955                1\n 9            5201              523             1955                1\n10            5201              523             1955                1\n# … with 19,608 more rows, and 8 more variables: type <chr>,\n#   trait <chr>, item <chr>, year <chr>, value <dbl>, reverse <dbl>,\n#   mini <dbl>, maxi <dbl>\n\n\n# A tibble: 15,061 x 3\n# Groups:   Procedural__SID [10,019]\n   Procedural__SID trait     value\n             <dbl> <chr>     <dbl>\n 1             901 MomDied       1\n 2            2301 MoveIn        0\n 3            2301 PartDied      1\n 4            2305 MoveIn        0\n 5            4601 PartDied      0\n 6            5201 ChldMvOut     1\n 7            5201 DadDied       0\n 8            5202 ChldMvOut     1\n 9            5203 MoveIn        1\n10            5303 MomDied       0\n# … with 15,051 more rows\n\nFor count variables, like life events, we need to use something slightly different. We’re typically more interested in counts – in this case, how many people experienced each life event in the 10 years we’re considering?\nTo do this, we’ll use a little bit of dplyr rather than the base R function table() that is often used for count data. Instead, we’ll use a combination of group_by() and n() to get the counts by group. In the end, we’re left with a nice little table of counts.\n\n# A tibble: 20 x 3\n# Groups:   trait [10]\n   trait     value     N\n   <chr>     <dbl> <int>\n 1 ChldBrth      0  1600\n 2 ChldBrth      1   735\n 3 ChldMvOut     0  1555\n 4 ChldMvOut     1   830\n 5 DadDied       0   953\n 6 DadDied       1   213\n 7 Divorce       0   414\n 8 Divorce       1   122\n 9 Married       0  1646\n10 Married       1   331\n11 MomDied       0   929\n12 MomDied       1   219\n13 MoveIn        0  1403\n14 MoveIn        1   419\n15 NewPart       0  1207\n16 NewPart       1   420\n17 PartDied      0   402\n18 PartDied      1    76\n19 SepPart       0  1172\n20 SepPart       1   415\n\n\n# A tibble: 10 x 3\n   trait       `0`   `1`\n   <chr>     <int> <int>\n 1 ChldBrth   1600   735\n 2 ChldMvOut  1555   830\n 3 DadDied     953   213\n 4 Divorce     414   122\n 5 Married    1646   331\n 6 MomDied     929   219\n 7 MoveIn     1403   419\n 8 NewPart    1207   420\n 9 PartDied    402    76\n10 SepPart    1172   415\n\nZero-Order Correlations\nFinally, we often want to look at the zero-order correlation among study variables to make sure they are performing as we think they should.\nTo run the correlations, we will need to have our data in wide format, so we’re going to do a little bit of reshaping before we do.\n\n         DOB   Sex A_2005 A_2009 A_2013 C_2005 C_2009 C_2013 E_2005\nDOB     1.00  0.00  -0.08  -0.07  -0.06  -0.13  -0.12  -0.14   0.10\nSex     0.00  1.00   0.18   0.17   0.18   0.05   0.07   0.09   0.08\nA_2005 -0.08  0.18   1.00   0.50   0.50   0.32   0.20   0.19   0.10\nA_2009 -0.07  0.17   0.50   1.00   0.55   0.19   0.28   0.18   0.05\nA_2013 -0.06  0.18   0.50   0.55   1.00   0.18   0.19   0.29   0.04\nC_2005 -0.13  0.05   0.32   0.19   0.18   1.00   0.52   0.48   0.19\nC_2009 -0.12  0.07   0.20   0.28   0.19   0.52   1.00   0.55   0.12\nC_2013 -0.14  0.09   0.19   0.18   0.29   0.48   0.55   1.00   0.13\nE_2005  0.10  0.08   0.10   0.05   0.04   0.19   0.12   0.13   1.00\nE_2009  0.12  0.08   0.06   0.08   0.06   0.10   0.16   0.14   0.61\nE_2013  0.10  0.11   0.04   0.04   0.07   0.10   0.10   0.18   0.59\nN_2005  0.06 -0.18   0.10   0.06   0.02   0.09   0.06   0.03   0.18\nN_2009  0.03 -0.22   0.07   0.09   0.03   0.06   0.08   0.05   0.13\nN_2013  0.02 -0.21   0.06   0.06   0.10   0.04   0.06   0.08   0.10\nO_2005  0.11  0.06   0.12   0.09   0.07   0.17   0.12   0.08   0.40\nO_2009  0.10  0.05   0.05   0.11   0.07   0.06   0.14   0.08   0.26\nO_2013  0.05  0.07   0.08   0.09   0.13   0.07   0.08   0.15   0.24\n       E_2009 E_2013 N_2005 N_2009 N_2013 O_2005 O_2009 O_2013\nDOB      0.12   0.10   0.06   0.03   0.02   0.11   0.10   0.05\nSex      0.08   0.11  -0.18  -0.22  -0.21   0.06   0.05   0.07\nA_2005   0.06   0.04   0.10   0.07   0.06   0.12   0.05   0.08\nA_2009   0.08   0.04   0.06   0.09   0.06   0.09   0.11   0.09\nA_2013   0.06   0.07   0.02   0.03   0.10   0.07   0.07   0.13\nC_2005   0.10   0.10   0.09   0.06   0.04   0.17   0.06   0.07\nC_2009   0.16   0.10   0.06   0.08   0.06   0.12   0.14   0.08\nC_2013   0.14   0.18   0.03   0.05   0.08   0.08   0.08   0.15\nE_2005   0.61   0.59   0.18   0.13   0.10   0.40   0.26   0.24\nE_2009   1.00   0.65   0.10   0.16   0.10   0.29   0.36   0.28\nE_2013   0.65   1.00   0.11   0.13   0.15   0.26   0.28   0.35\nN_2005   0.10   0.11   1.00   0.55   0.53   0.09   0.08   0.06\nN_2009   0.16   0.13   0.55   1.00   0.60   0.06   0.07   0.07\nN_2013   0.10   0.15   0.53   0.60   1.00   0.05   0.05   0.05\nO_2005   0.29   0.26   0.09   0.06   0.05   1.00   0.58   0.55\nO_2009   0.36   0.28   0.08   0.07   0.05   0.58   1.00   0.61\nO_2013   0.28   0.35   0.06   0.07   0.05   0.55   0.61   1.00\n\n\n\n\n",
      "last_modified": "2021-01-13T13:46:35-06:00"
    },
    {
      "path": "index.html",
      "title": "Applied Longitudinal Data Analysis",
      "description": "Updates and Announcements",
      "author": [],
      "contents": "\ntest\nTest again.\nThrice for good luck.\n\n\n\n",
      "last_modified": "2021-01-13T13:46:36-06:00"
    },
    {
      "path": "intro-1.html",
      "title": "Intro to longitudinal thinking",
      "author": [],
      "contents": "\n    \nclass: center, middle, inverse, title-slide\n\n# Intro to longitudinal thinking\n\n---\n\n\n\nApplied Longitudinal Data Analysis\n\n\nHome\n\n\nLectures\n \n▾\n\n\nIntro to longitudinal\nMLM growth models\n\n\n\n\nWorkshops\n \n▾\n\n\ndata\nlme4, nlme\nworkflow\nplotting\nsimulation\npurr\nbrms\nlavaan\nmice\n\n\nSyllabus\nHomeworks\n\n\n\n☰\n\n\n\n\nGoals for today: \n1. Get a feeling for how to think/talk about longitudinal/repeated measures data\n2. Introduce some important terms\n3. Begin to develop a framework for analysis\n\n---\n## how to think longitudinal-y\n\n.pull-left[\n1. lines/trajectories \n\n2. variance decomposition \n]\n\n.pull-right[\n![](intro-1_files/figure-html/unnamed-chunk-2-1.png)<!-- -->\n]\n\n---\n## Types of change (most common)\n\nDifferential / rank order consistency/rank order stability. \n\nMean level/ absolute change. \n\n---\n.pull-left[\nPerfect rank order, mean level increase\n\n]\n\n.pull-right[\nNo rank order, mean level increase\n![](intro-1_files/figure-html/unnamed-chunk-4-1.png)<!-- -->\n]\n\n---\n### types of change cont\n\nIndividual differences in change. \n\nStructural.\n\nVariance (or parameters other than location).\n\nIpsative\n\n---\n## Be precise about change and stability\n\n(Usually) it clearer to refer to the type of change in terms of an equation or pictorially. Putting a word onto it usually causes some confusion, which is why there are a lot of redundant terms in the literature. \n\n---\n## Thought example\n\nThink of your construct of choice. Can you describe at least 4 ways it can \"change\"\n\n---\n## Useful qualities\n\n- Interval or greater scale of measurement. We can measures change on non interval or ratio scale but it is more difficult. \n\n- Construct has the same meaning across measurement occasions. Usually the same items. Called measurement invariance. Complicates developmental work\n\n- 2 or more measurement occasions. More is better! Though often 3 - 10 is practically fine (More is better in terms of people and indicators, too) \n\n---\n# Defining time metric\n\nTime is the most important part of a longitudinal analyses. The key to interpreting your output is to know how you handled your time variable. \n\nWhat is the process that is changing someone? Age? Time in study? Year? Wave? \n\nIs it a naturally occurring developmental process? Then maybe age is the best metric. What about tracking child's cognitive ability, something that might be influenced by level of schooling? Here grade may be more important than age. If you are running an intervention you may want to put everyone on the same starting metric and then control for nuisance variables like age or schooling level.   \n\n---\n## Example\n.pull-left[\nUsing some resting state imaging data, lets think about how we can model and think about this data using our current skills (ie standard regression and plotting)\n\n\n\n\n\n```r\ngg1 <- ggplot(example,\n   aes(x = year, y = SMN7, group = ID)) + geom_point()  \nprint(gg1)\n```\n]\n\n.pull-right[\nWe defined time as year in study. How would this look if we used age? \n![](intro-1_files/figure-html/unnamed-chunk-7-1.png)<!-- -->\n]\n\n---\n.pull-left[\nDo we have repeated assessments per person? Lets find out. \n\n\n```r\nlibrary(viridis)\n ggplot(example,\n   aes(x = year, y = SMN7, group = ID, colour = ID)) + geom_line(alpha = .4) +scale_color_viridis(   )\n```\n]\n\n.pull-right[\n\n![](intro-1_files/figure-html/unnamed-chunk-9-1.png)<!-- -->\n\nNote that some people start at different levels. Some people have more data in terms of assessment points and years. Also, the shape of change isn't necessarily a straight line. \n\n]\n\n---\nWe often want to look at this at a per person level to get more info. \n\n```r\nggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_line() +  geom_point() + facet_wrap( ~ ID)\n```\n\n![](intro-1_files/figure-html/unnamed-chunk-10-1.png)<!-- -->\n\n---\n.pull-left[\nAs part of our dataset we have different groups. A question we may have is if they change differently across time. Lets take a look at this. \n\n```r\nggplot(example,\n   aes(x = year, y = SMN7, group = ID)) + geom_line() + facet_grid(. ~ group)\n```\n]\n\n.pull-right[\n\n![](intro-1_files/figure-html/unnamed-chunk-12-1.png)<!-- -->\n\n\n\n]\n\n\n---\n.pull-left[\nBeside the occular technique, we're going to need to do something more to address our theoretical questions. Lets look at some random people in the sample and run some regressions. \n\n```r\nset.seed(11)\nex.random <- example %>% \n  dplyr::select(ID) %>% \n  distinct %>% \n  sample_n(10) \n\nexample2 <-\n  left_join(ex.random, example)  \n  \nggplot(example2,\n   aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method=\"lm\") + facet_wrap( ~ID)\n```\n]\n\n.pull-right[\n\n![](intro-1_files/figure-html/unnamed-chunk-14-1.png)<!-- -->\n]\n\n\n---\nLets look at individual level regressions\n\n\n```r\nlibrary(tidyverse)\nlibrary(broom)\n\nregressions <- example2 %>% \n  group_by(ID) %>% \n  do(tidy(lm(SMN7 ~ week, data=.)))\n\nregressions\n```\n\n```\n## # A tibble: 20 x 6\n## # Groups:   ID [10]\n##       ID term        estimate std.error statistic  p.value\n##    <dbl> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n##  1    67 (Intercept)  0.0921    0.0161      5.72    0.0292\n##  2    67 week         0.00662   0.00657     1.01    0.420 \n##  3    75 (Intercept)  0.126   NaN         NaN     NaN     \n##  4    75 week         0.00771 NaN         NaN     NaN     \n##  5    87 (Intercept)  0.0787  NaN         NaN     NaN     \n##  6    87 week        -0.0227  NaN         NaN     NaN     \n##  7    99 (Intercept)  0.111     0.0236      4.69    0.0426\n##  8    99 week         0.00545   0.0122      0.446   0.699 \n##  9   101 (Intercept)  0.111     0.0424      2.62    0.232 \n## 10   101 week         0.0421    0.0217      1.94    0.303 \n## 11   103 (Intercept)  0.0887  NaN         NaN     NaN     \n## 12   103 week        -0.0168  NaN         NaN     NaN     \n## 13   105 (Intercept)  0.0465    0.00658     7.06    0.0896\n## 14   105 week         0.00122   0.00467     0.261   0.838 \n## 15   142 (Intercept)  0.197   NaN         NaN     NaN     \n## 16   142 week         0.0130  NaN         NaN     NaN     \n## 17   149 (Intercept)  0.0801  NaN         NaN     NaN     \n## 18   149 week         0.00497 NaN         NaN     NaN     \n## 19   152 (Intercept)  0.0921  NaN         NaN     NaN     \n## 20   152 week        -0.0172  NaN         NaN     NaN\n```\n\n---\nWhat can we see? Estimates give us an intercept and regression coefficient for each person. Some people increase across time, some decrease. Some we cannot do statistical tests on -- why? \n\nWell that is per person. Lets get the average starting value and change per week\n\n```r\nregressions %>% \n  group_by(term) %>% \n  summarise(avg.reg = mean(estimate))\n```\n\n```\n## `summarise()` ungrouping output (override with `.groups` argument)\n```\n\n```\n## # A tibble: 2 x 2\n##   term        avg.reg\n##   <chr>         <dbl>\n## 1 (Intercept) 0.102  \n## 2 week        0.00244\n```\n\n---\nLets plot the average trend across everyone. Start with a best fit line not taking into account that people have repeated measures. \n\n\n```r\nggplot(example, aes(x = year, y = SMN7)) + geom_point() + stat_smooth() \n```\n\n![](intro-1_files/figure-html/unnamed-chunk-17-1.png)<!-- -->\n\n\n--- \n\n\n\n```r\nggplot(example, aes(x = year, y = SMN7)) + geom_point() + stat_smooth(method = \"lm\") \n```\n\n![](intro-1_files/figure-html/unnamed-chunk-18-1.png)<!-- -->\n\n---\n\n```r\nggplot(example, aes(x = year, y = SMN7)) + geom_point() + stat_smooth(method = \"lm\") + facet_grid(. ~ group)\n```\n\n![](intro-1_files/figure-html/unnamed-chunk-19-1.png)<!-- -->\n\n---\n.pull-left[\n\n```r\ngg9 <- ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_point(alpha = 0.05) + stat_smooth(method = \"lm\", se = FALSE)   \ngg10 <- gg9 +  stat_smooth(data = example, aes(x = year, y = SMN7, group=1, color = \"black\"), method = \"lm\", size = 2) + guides(fill=FALSE)\n\ngg11 <- gg10 + facet_grid(.~ group) + theme(legend.position=\"none\")\n\ngg11\n```\n]\n\n.pull-right[\n\n![](intro-1_files/figure-html/unnamed-chunk-21-1.png)<!-- -->\n\n]\n\n\n---\n## Using MLM to examine \naka HLM, aka mixed effects, aka random effects.  \n\nWhy? BC we would 1. violate standard regression assumptions. 2. because of the flexibility\n\nSplits the model into two components:   \nmean and variance   \nlocation and scale  \nfixed and random  \nconstant (across people) and varying (across people)  \n\n---\n## Terminology again\n\nGeneral Linear Models  \nGeneralIZED Linear Models  \nGeneral Linear Mixed Models  \nGeneralIZED Linear Mixed Models  \n  \nMixed means both fixed and random effects   \n\"...IZED\" means gaussian and other data generating processes \n\n---\n## what about SEM? \nMLM and SEM can be equivalent. \n\nWe will start with MLM/HLM is a simple extension of standard regression models. Best suited to run models when the time of measurement differs from person to person (compared to equal intervals). MLM is also better suited for complex error structures and complex nesting above and beyond assessments within person\n\nSEMs two primary advantages are the ability to account for measurement error via latent variables and incorporating multiple DVs. \n\n---\n## Why not RM ANOVA? \ntl;dr: it is less flexible. Antiquated method. \n1. Cannot handle missing data\n2. Assumes rate of change is the same for all individuals.\n3. Time is categorical. \n4. Accounting for correlation across time uses up many parameters (df penalty).\n5. Handles various types of predictors - continuous vs nominal & static vs dynamic\n6. Special case of MLM, might as well learn/use flexible model\n\n---\n## Modeling dependency\n\nWe have multiple DVs per person with longitudinal data. If we ignored the person aspect, the residuals would likely be related, violating standard regression assumption. MLM accounts for residuals for outcomes from the same person through modeling different \"levels\" (of random effects).\n\nTo have a “level”, there must be random outcome variation left over. With longitudinal data we have people nested in observations. \n\nLevel 1: observation level (observation specific variance)  \nLevel 2: person level (person specific variance)  \n\n---\n### Person specific variance\n.pull-left[\nSome people start at different levels and some people change at different rates\n]\n\n.pull-right[\n![](intro-1_files/figure-html/unnamed-chunk-22-1.png)<!-- -->\n]\n\n---\n### Observation level variance\n.pull-left[\nAfter account for a person starting level and their slope, there is still residual variance left over. \n]\n\n.pull-right[\n![](intro-1_files/figure-html/unnamed-chunk-23-1.png)<!-- -->\n]\n\n\n---\n## Thinking about variation \nA goal of longitudinal data analysis (and all other data analysis) is to explain this variation. We will fit models that are more constrained (makes assumptions about the shape of change across people) to see if that increases or reduces variation at the two levels. \n\nTo the extent that we can put variance into different \"piles\" (eg people change at different rates) we will have more explained variance and less unexplained variance. Less unexplained variance means our model fits better ie it represents our data better\n\n---\n## Speaking of variation \n\nBetween-Person (BP) Variation:  \nLevel-2 – “INTER-individual Differences” - Time-Invariant   \nBP = More/less than other people  \n\n\nWithin-Person (WP) Variation:  \nLevel-1 – “INTRA-individual Differences” – Time-Varying  \nWP = more/less than one’s average  \n\nAny variable measured over time usually has both BP and WP variation. People who vary in positive emotion may be higher in PE. If we did not separate the two and only looked at WP then we will introduce bias. \n\n\n---\n### Within person focus\n\nWithin-Person Change: Systematic change\nMagnitude or direction of change can be different across individuals. Can refer to between person (inter individual differences) in within person change (intra individual)\n\nWithin-Person Fluctuation: No systematic change\nOutcome just varies/fluctuates over time (e.g., emotion, stress). Time is just a way to get lots of data per individual\n\n\n---\n\n---\n## Why longitudinal? \nAt least 7 reasons:\n\n1. Identification of intraindividual change (and stability). Do people increase or decrease with time or age. Is this pattern monotonic? Should this best be conceptualized as a stable process or something that is more dynamic? On average how do people change? Ex: people decline in cognitive ability across time. \n\n2. Inter-individual differences in intraindividual change. Does everyone change the same? Do some people start higher but change less? Do some increase while some decrease? Ex: not all people people decline in cognitive ability across time, but some do. \n\n3. Examine joint relationship among intraindividual change for two or more constructs. If variable X goes up does variable Y also go up across time? Does this always happen or only during certain times? Is this association due to a third variable or does it mean that change occurs for similar reasons? Ex: changes in cognitive ability are associated with changes in health across time.   \n\n---\n\n4. Determinants of intraindividual change. What are the repeated experiences that can push construct X around. Do these have similar effects at all times? Ex: people declinein cognitive ability across time. Ex: I have better memory compared to other times when I engage in cognitive activities vs times that I do not.   \n\n5. Determinants of interindividual differences in intraindividual change. Do events, background characteristics, interventions or other between person characteristic shape why certain people change while others don't? Ex: people decline less in cognitive ability across time if tend to do cognitively engaging activities.  \n\n6. Inter-individual differences in intraindividual fluctuation and determinants of intraindividual fluctuation. Does everyone vary the same? Why are some more variable than others? Ex: Someone who is depressed fluctuates more in happiness than someone who is not depressed\n\n7. Are there different classes/populations/mixtures of intraindividual change? Ex: do people who decrease vs don't in cognitive ability across time exist as different groups? (Vs construing differences as on a continuum). \n\n\n---\n## Design considerations\n\n1. Number of assessment waves\n\nRemember high school algebra: two points define a line. But, that assumes we can measure constructs without error. Three assessment points will better define changes in psychological variables. As a default, you need three waves of data to use MLM models. However, some simplifications can be made with MLM. Two wave assessments are mostly better with SEM approaches. \n\n---\n2. Scale of measurement\nMeasurement is always the basis for good quantitative analysis. Without good measurement you are just spitting into the wind. Standard measurement concerns remain (reliability, dimensionality) but extra concerns exist with longitudinal data. \n\nWhat does it mean for categorical variables to change over time? Can you imagine a trajectory for what this is measuring? \n\nWhat about ranks, such as in preference for school subjects? What if the class composition changes -- what is this assessing? Given that ranks are related such that if I increase someone has to decrease, how does that impact change assessments?   \n\nCan I analyze childhood and adult variables simultaneously if assess the same construct, even though they may be measured differently? How can you measure change in the same construct but with different measures? To assess math ability in 5 year olds you can ask them about addition, can you do that in a sample of 20 year olds? \n\n\n---\n3. Standardizing\nIt is standard practice to z-score to get standardized responses. However, it is not straight forward to do so when using longitudinal data. Why would z-scoring your variables be problematic? \n\nFirst, if you scale for age, for example, this takes out a potential explanatory variable. \n\nSecond, it also can add error if not everyone is standardized consistently (say if standardization is across age groups and someone just misses a cut). \n\nThird, is that you take away the mean for each assessment such that the expected change across time is zero.  \n\n---\n4. Reliability \n\nThe goal of longitudinal analyses is to understand why some construct changes or stays the same across time. A major difficulty in addressing this goal is whether you are able to accurately assess the construct of interest. To the extent that your measure is reliable it assesses true score variance as opposed to error variance. The amount of error score variance assessed is important given that error variance will masquerade as change across time.\n\n\nReliability of the change estimate depends on how much error there is in the assessment and the number of waves. These two components are similar to inter item correlation and number of items being the two main components that effect reliability in cross sectional analyses. \n\n---\n5. Measurement invariance\n\nDo you assess the same construct at each time? What would happen if we looked at change in IQ from 1st grade to 12 grade and used the first grade IQ test at each time? The construct that you assessed at the first wave is likely not the same assessed later. \n\nTo test this formally is called measurement invariance and is typically done through SEM.  Often there is a large assumption that what we are measuring now is the same at each wave of assessment.  \n\n\n---\n## Threats to validity\n\n1. Missing data\nMissing completely at random (MCAR) means that the missingness pattern is due entirely to randomness\n\nMissing at random (MAR) means that there is conditional randomness. Missingness may be due to other variables in the dataset. \n\nNot missing at random (NMAR) means that the missingness is systematic based on the missing values and not associated with measured variables. For example, in a study of reading ability, kids with low reading ability drop out, due to not liking to take tests on reading ability. \n\nTypically, we make the assumption we are working under MAR and thus we will have unbiased estimates when predictors of missingness are incorporated into the model. \n\n---\n2. Attrition/Mortality\nMajor contributor to missing data\n\n3. History/cohort effects\nKnow that the processes driving change can be due to a specific event or cohort.\n\n4. Maturation \nChange may occur because of natural processes. Thus if you just follow someone across time they will likely change irregardless of say, if they are in the control group.\n\n---\n5. Testing \nHaving people take the same survey, test or interview multiple times may lead them to respond differently. Does that change result from development or does it result from them being familiar with the test? \n\n6. Selection \nIf you are looking at life events, know that life events are not distributed randomly. Moreover, people who stay in studies and even sign up for studies are different from those that do not. As a result, it is often hard to make internally valid inferences with longitudinal data. \n\n\n    @media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}var slideshow = remark.create();\nif (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {\n  window.dispatchEvent(new Event('resize'));\n});\n(function(d) {\n  var s = d.createElement(\"style\"), r = d.querySelector(\".remark-slide-scaler\");\n  if (!r) return;\n  s.type = \"text/css\"; s.innerHTML = \"@page {size: \" + r.style.width + \" \" + r.style.height +\"; }\";\n  d.head.appendChild(s);\n})(document);\n\n(function(d) {\n  var el = d.getElementsByClassName(\"remark-slides-area\");\n  if (!el) return;\n  var slide, slides = slideshow.getSlides(), els = el[0].children;\n  for (var i = 1; i < slides.length; i++) {\n    slide = slides[i];\n    if (slide.properties.continued === \"true\" || slide.properties.count === \"false\") {\n      els[i - 1].className += ' has-continuation';\n    }\n  }\n  var s = d.createElement(\"style\");\n  s.type = \"text/css\"; s.innerHTML = \"@media print { .has-continuation { display: none; } }\";\n  d.head.appendChild(s);\n})(document);\n// delete the temporary CSS (for displaying all slides initially) when the user\n// starts to view slides\n(function() {\n  var deleted = false;\n  slideshow.on('beforeShowSlide', function(slide) {\n    if (deleted) return;\n    var sheets = document.styleSheets, node;\n    for (var i = 0; i < sheets.length; i++) {\n      node = sheets[i].ownerNode;\n      if (node.dataset[\"target\"] !== \"print-only\") continue;\n      node.parentNode.removeChild(node);\n    }\n    deleted = true;\n  });\n})();\n(function() {\n  \"use strict\"\n  // Replace <script> tags in slides area to make them executable\n  var scripts = document.querySelectorAll(\n    '.remark-slides-area .remark-slide-container script'\n  );\n  if (!scripts.length) return;\n  for (var i = 0; i < scripts.length; i++) {\n    var s = document.createElement('script');\n    var code = document.createTextNode(scripts[i].textContent);\n    s.appendChild(code);\n    var scriptAttrs = scripts[i].attributes;\n    for (var j = 0; j < scriptAttrs.length; j++) {\n      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);\n    }\n    scripts[i].parentElement.replaceChild(s, scripts[i]);\n  }\n})();\n(function() {\n  var links = document.getElementsByTagName('a');\n  for (var i = 0; i < links.length; i++) {\n    if (/^(https?:)?\\/\\//.test(links[i].getAttribute('href'))) {\n      links[i].target = '_blank';\n    }\n  }\n})();\nslideshow._releaseMath = function(el) {\n  var i, text, code, codes = el.getElementsByTagName('code');\n  for (i = 0; i < codes.length;) {\n    code = codes[i];\n    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {\n      text = code.textContent;\n      if (/^\\\\\\((.|\\s)+\\\\\\)$/.test(text) || /^\\\\\\[(.|\\s)+\\\\\\]$/.test(text) ||\n          /^\\$\\$(.|\\s)+\\$\\$$/.test(text) ||\n          /^\\\\begin\\{([^}]+)\\}(.|\\s)+\\\\end\\{[^}]+\\}$/.test(text)) {\n        code.outerHTML = code.innerHTML;  // remove <code><\/code>\n        continue;\n      }\n    }\n    i++;\n  }\n};\nslideshow._releaseMath(document);\n\n(function () {\n  var script = document.createElement('script');\n  script.type = 'text/javascript';\n  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';\n  if (location.protocol !== 'file:' && /^https?:/.test(script.src))\n    script.src  = script.src.replace(/^https?:/, '');\n  document.getElementsByTagName('head')[0].appendChild(script);\n})();\n",
      "last_modified": "2021-01-13T13:46:52-06:00"
    }
  ],
  "collections": []
}
